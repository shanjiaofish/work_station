# work_station/backend/app.py

# --- åŸºç¤èˆ‡ Web å¥—ä»¶ ---
import os
import sys
import time
import datetime
import io
import zipfile
import re
import shutil
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, request, send_from_directory, send_file
from flask_cors import CORS
from flask_restx import Api, Resource, fields, reqparse
import werkzeug.utils
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# --- åŠŸèƒ½æ€§å¥—ä»¶ ---
try:
    import googlemaps
except ImportError as e:
    print(f"Warning: {e}. Some features may not work.")
    googlemaps = None

try:
    from supabase_client import supabase
except ImportError as e:
    print(f"Warning: {e}. Some features may not work.")
    supabase = None

try:
    from gmap_robot import GoogleMapsRobot
except ImportError as e:
    print(f"Warning: {e}. Some features may not work.")
    GoogleMapsRobot = None

# --- OCR ç›¸é—œå¥—ä»¶ (å»¶é²å°å…¥ä»¥åŠ å¿«å•Ÿå‹•) ---
# Import these only when OCRåŠŸèƒ½ is actually needed
# This prevents blocking the app startup with large model downloads
try:
    import cv2
    import numpy as np
    from pdf2image import convert_from_path
    # å¾åŸæœ¬çš„ OCR å·¥å…·å°å…¥è¨­å®šè®Šæ•¸
    # è«‹ç¢ºä¿ param.py èˆ‡ app.py åœ¨åŒä¸€å€‹è³‡æ–™å¤¾ä¸­
    from param import *
    OCR_IMPORTS_AVAILABLE = True
except ImportError as e:
    print(f"Warning: OCR dependencies not fully available: {e}")
    OCR_IMPORTS_AVAILABLE = False

# Lazy import OCR engines - these will be imported only when init_ocr_engines() is called
CnOcr = None
easyocr = None
PaddleOCR = None

# --- è§£æ±º Flask åœ¨ Windows ä¸­ print() å¯èƒ½ç”¢ç”Ÿçš„äº‚ç¢¼å•é¡Œ ---
try:
    sys.stdout.reconfigure(encoding='utf-8')
except AttributeError:
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ======================================================================
# --- æ‡‰ç”¨ç¨‹å¼åˆå§‹åŒ–èˆ‡è¨­å®š (åªéœ€ä¸€æ¬¡) ---
# ======================================================================
app = Flask(__name__)
CORS(app, 
     origins=['http://localhost:5173', 'http://localhost:5174', 'http://127.0.0.1:5173', 'http://127.0.0.1:5174'],
     methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
     allow_headers=['Content-Type', 'Authorization'])

# Swagger/OpenAPI configuration
api = Api(
    app,
    version='1.0',
    title='MFish Station Backend API',
    description='Backend API for MFish Station - OCR, Google Maps, and Material Management',
    doc='/docs/',
    prefix='/api'
)

# --- è·¯å¾‘è¨­å®š ---
basedir = os.path.abspath(os.path.dirname(__file__))
app.config['SCREENSHOTS_FOLDER'] = os.path.join(basedir, 'screenshots')
app.config['UPLOAD_FOLDER'] = os.path.join(basedir, 'uploads')
app.config['REPORTS_FOLDER'] = os.path.join(basedir, 'reports')
app.config['TEMP_IMG_FOLDER'] = os.path.join(basedir, 'temp_imgs')
app.config['CROPPED_RECEIPTS_FOLDER'] = os.path.join(basedir, 'cropped_receipts')

# --- å…¨åŸŸç‰©ä»¶ ---
GOOGLE_MAPS_API_KEY = os.getenv("MAPS_API_KEY")
gmaps = googlemaps.Client(key=GOOGLE_MAPS_API_KEY) if GOOGLE_MAPS_API_KEY and googlemaps else None
SESSION_RESULTS_CACHE = {}

# OCR å¼•æ“ (å»¶é²åˆå§‹åŒ–)
ocr_engines = { "cnocr": None, "easyocr": None, "paddleocr": None }

# ======================================================================
# --- OCR æ ¸å¿ƒé‚è¼¯ (100% ç§»æ¤è‡ª gas_helper.py) ---
# ======================================================================

def init_ocr_engines():
    """åˆå§‹åŒ–æ‰€æœ‰ OCR å¼•æ“ã€‚"""
    global CnOcr, easyocr, PaddleOCR

    if ocr_engines["cnocr"] is None:
        print("é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨åˆå§‹åŒ– OCR å¼•æ“ (å¯èƒ½éœ€è¦å¹¾åˆ†é˜)...")

        # Lazy import OCR libraries here (not at module level)
        if CnOcr is None:
            try:
                from cnocr import CnOcr as CnOcrClass
                CnOcr = CnOcrClass
                print("CnOCR library imported successfully")
            except ImportError as e:
                print(f"Failed to import CnOCR: {e}")

        if easyocr is None:
            try:
                import easyocr as easyocr_module
                easyocr = easyocr_module
                print("EasyOCR library imported successfully")
            except ImportError as e:
                print(f"Failed to import EasyOCR: {e}")

        if PaddleOCR is None:
            try:
                from paddleocr import PaddleOCR as PaddleOCRClass
                PaddleOCR = PaddleOCRClass
                print("PaddleOCR library imported successfully")
            except ImportError as e:
                print(f"Failed to import PaddleOCR: {e}")

        # åˆå§‹åŒ– CnOcr - ä½¿ç”¨ try/except è™•ç†å¯èƒ½çš„å°å…¥å•é¡Œ
        if CnOcr:
            try:
                ocr_engines["cnocr"] = CnOcr()
                print("CnOCR åˆå§‹åŒ–æˆåŠŸï¼")
            except Exception as e:
                print(f"CnOCR åˆå§‹åŒ–å¤±æ•—: {e}")
                ocr_engines["cnocr"] = None
        else:
            print("CnOCR not available, skipping")

        # åˆå§‹åŒ– EasyOCR
        if easyocr:
            try:
                ocr_engines["easyocr"] = easyocr.Reader(['ch_tra', 'en'])
                print("EasyOCR åˆå§‹åŒ–æˆåŠŸï¼")
            except Exception as e:
                print(f"EasyOCR åˆå§‹åŒ–å¤±æ•—: {e}")
                ocr_engines["easyocr"] = None
        else:
            print("EasyOCR not available, skipping")

        # åˆå§‹åŒ– PaddleOCR - è™•ç†ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        if PaddleOCR:
            try:
                # å…ˆå˜—è©¦ä¸ä½¿ç”¨å·²æ£„ç”¨çš„åƒæ•¸
                ocr_engines["paddleocr"] = PaddleOCR(use_textline_orientation=False, lang='ch')
                print("PaddleOCR åˆå§‹åŒ–æˆåŠŸï¼")
            except Exception as e:
                print(f"PaddleOCR åˆå§‹åŒ–å¤±æ•— (æ–°ç‰ˆ): {e}")
                try:
                    # å›é€€åˆ°èˆŠç‰ˆåƒæ•¸
                    ocr_engines["paddleocr"] = PaddleOCR(use_angle_cls=False, lang='ch')
                    print("PaddleOCR åˆå§‹åŒ–æˆåŠŸ (èˆŠç‰ˆ)ï¼")
                except Exception as e2:
                    print(f"PaddleOCR å®Œå…¨åˆå§‹åŒ–å¤±æ•—: {e2}")
                    ocr_engines["paddleocr"] = None
        else:
            print("PaddleOCR not available, skipping")

        print("OCR å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")

def detect_invoices_from_pdf(pdf_path: str) -> list:
    """å¾ PDF ä¸­åˆ†å‰²å‡ºæ‰€æœ‰ç™¼ç¥¨åœ–ç‰‡ã€‚"""
    os.makedirs(app.config['TEMP_IMG_FOLDER'], exist_ok=True)
    os.makedirs(app.config['CROPPED_RECEIPTS_FOLDER'], exist_ok=True)
    invoice_images = []
    images = convert_from_path(pdf_path, dpi=300)
    for i, img in enumerate(images):
        page_filename = f'page_{i + 1}.png'
        img_path = os.path.join(app.config['TEMP_IMG_FOLDER'], page_filename)
        img.save(img_path, 'PNG')
        image = cv2.imread(img_path)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        bin_img = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 25, 15)
        height, width = gray.shape[:2]
        scale = max(width, height) / 1000
        ksize = int(30 * scale)
        ksize = max(20, min(ksize, 80))
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (ksize, ksize))
        dilated = cv2.dilate(bin_img, kernel, iterations=1)
        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        boxes = [cv2.boundingRect(c) for c in contours if cv2.contourArea(c) > 5000]
        boxes = sorted(boxes, key=lambda b: (b[1], b[0]))
        for j, (x, y, w, h) in enumerate(boxes):
            crop_img = image[y:y + h, x:x + w]
            crop_path = os.path.join(app.config['CROPPED_RECEIPTS_FOLDER'], f'{page_filename}_block{j}.png')
            cv2.imwrite(crop_path, crop_img)
            invoice_images.append(crop_path)
    return invoice_images

def detect_fuel_type(text_combined: str) -> str:
    """å¾æ–‡å­—ä¸­åµæ¸¬ç‡ƒæ²¹ç¨®é¡ã€‚"""
    matches = []
    for wrong, correct in fuel_fuzzy_mapping.items():
        text_combined = text_combined.replace(wrong, correct)
    for fuel in fuel_keywords:
        if fuel in text_combined:
            mapped = fuel_mapping.get(fuel, fuel)
            matches.append((fuel, mapped))
    if matches:
        return sorted(matches, key=lambda x: -len(x[0]))[0][1]
    return None

def extract_invoice_info(img_path: str) -> dict:
    """å¾å–®å¼µç™¼ç¥¨åœ–ç‰‡ä¸­æ“·å–è³‡è¨Š (å®Œæ•´ç‰ˆ)ã€‚"""
    cnocr_lines = [''.join(block['text']) for block in ocr_engines["cnocr"].ocr(img_path)]
    zh_lines = ocr_engines["easyocr"].readtext(img_path, detail=0)
    all_lines = cnocr_lines + zh_lines
    invoice_number, date, quantity, fuel_type, address = None, None, None, None, None

    for line in cnocr_lines:
        if not invoice_number and (match := invoice_number_pattern.search(line)): invoice_number = match.group()
        if not date and (match := date_pattern.search(line)): date = match.group()

    for i, line in enumerate(all_lines):
        if not quantity and any(k in line for k in fuel_keywords):
            if (match := quantity_pattern.search(line)): quantity = match.group(1); break
            if (match := quantity_fallback_pattern.search(line)): quantity = match.group(); break
            if i + 1 < len(all_lines):
                next_line = all_lines[i + 1]
                if (match := quantity_pattern.search(next_line)): quantity = match.group(1); break
                if (match := quantity_fallback_pattern.search(next_line)): quantity = match.group(); break

    if not quantity:
        for line in all_lines:
            if (match := quantity_pattern.search(line)): quantity = match.group(1); break

    all_text_combined = ' '.join(all_lines)
    fuel_type = detect_fuel_type(all_text_combined)

    for line in zh_lines:
        if not address and (match := address_pattern.search(line)): address = line; break
    if not address:
        for line in zh_lines:
            if any(keyword in line for keyword in district_keywords) and 'è™Ÿ' in line and re.search(r'\d+', line): address = line; break

    # --- é—œéµï¼šå®Œæ•´çš„ PaddleOCR å‚™ç”¨æ–¹æ¡ˆ ---
    if not all([invoice_number, date, quantity, fuel_type, address]) and ocr_engines["paddleocr"] is not None:
        paddle_result = ocr_engines["paddleocr"].ocr(img_path) # ä¿®æ­£ï¼šç§»é™¤ cls=False
        if paddle_result and paddle_result[0]:
            paddle_lines = [line[1][0] for line in paddle_result[0]]
            if not invoice_number:
                for line in paddle_lines:
                    if (match := invoice_number_pattern.search(line)): invoice_number = match.group(); break
            if not date:
                for line in paddle_lines:
                    if (match := date_pattern.search(line)): date = match.group(); break
            if not quantity:
                for line in paddle_lines:
                    if (match := simple_quantity_pattern.search(line)): quantity = match.group(1); break
            if not fuel_type:
                text_joined = ' '.join(paddle_lines)
                fuel_type = detect_fuel_type(text_joined)

    # --- é—œéµï¼šå®Œæ•´çš„è³‡æ–™æ¸…ç† ---
    if address:
        replacements = {'åŠç¦¹é”ˆå¨œ': 'è¬å·’é„‰', 'å·': 'è™Ÿ', 'é”ˆ': '', 'å¨œ': '', 'æ½®æ´²': 'æ½®å·', 'å·': 'å·', 'é–': 'é®'}
        for wrong, right in replacements.items(): address = address.replace(wrong, right)
    if invoice_number and not invoice_number_pattern.fullmatch(invoice_number): invoice_number = None
    if date and not date_pattern.fullmatch(date): date = None
    if quantity:
        try: float(quantity)
        except ValueError: quantity = None
    if fuel_type and fuel_type not in fuel_mapping.values(): fuel_type = None
    if address and (len(address) < 6 or 'è™Ÿ' not in address or not any(city in address for city in district_keywords)): address = None

    print(f"  > OCR çµæœ: {invoice_number}, {date}, {fuel_type}, {quantity}, {address}")
    return {
        'é æ•¸': os.path.basename(img_path), 'ç™¼ç¥¨è™Ÿç¢¼': invoice_number, 'æ—¥æœŸ': date,
        'ç¨®é¡': fuel_type, 'æ•¸é‡': quantity, 'åœ°å€': address, 'å‚™è¨»': ''
    }

def process_single_invoice_thread_safe(img_path: str, thread_id: int) -> dict:
    """ç·šç¨‹å®‰å…¨çš„å–®å¼µç™¼ç¥¨è™•ç†å‡½æ•¸ã€‚"""
    try:
        print(f"  ğŸ§µ Thread {thread_id}: Processing {os.path.basename(img_path)}")
        result = extract_invoice_info(img_path)
        print(f"  âœ… Thread {thread_id}: Completed {os.path.basename(img_path)} - Invoice: {result.get('ç™¼ç¥¨è™Ÿç¢¼', 'None')}")
        return result
    except Exception as e:
        print(f"  âŒ Thread {thread_id}: Error processing {os.path.basename(img_path)}: {e}")
        return {
            'é æ•¸': os.path.basename(img_path), 'ç™¼ç¥¨è™Ÿç¢¼': None, 'æ—¥æœŸ': None,
            'ç¨®é¡': None, 'æ•¸é‡': None, 'åœ°å€': None, 'å‚™è¨»': f'è™•ç†éŒ¯èª¤: {str(e)}'
        }

def process_invoice_pdf(pdf_path: str) -> tuple[str, list]:
    """æ•´åˆçš„å¤šç·šç¨‹ OCR è™•ç†æµç¨‹ï¼Œå›å‚³å ±å‘Šè·¯å¾‘å’Œçµæœè³‡æ–™ã€‚"""
    init_ocr_engines()
    print(f"æ­£åœ¨è™•ç† PDF: {pdf_path}")
    invoice_images = detect_invoices_from_pdf(pdf_path)
    num_invoices = len(invoice_images)
    print(f"åˆ†å‰²å‡º {num_invoices} å¼µç™¼ç¥¨ï¼Œé–‹å§‹å¤šç·šç¨‹ OCR è¾¨è­˜...")

    # å‹•æ…‹æ±ºå®šç·šç¨‹æ•¸é‡ï¼šæœ€å¤š4å€‹ç·šç¨‹ï¼Œä½†ä¸è¶…éç™¼ç¥¨æ•¸é‡
    max_workers = min(4, num_invoices, os.cpu_count() or 4)
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} å€‹ç·šç¨‹é€²è¡Œä¸¦è¡Œè™•ç†...")

    results = [None] * num_invoices  # é åˆ†é…çµæœåˆ—è¡¨ä»¥ä¿æŒé †åº
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_index = {
            executor.submit(process_single_invoice_thread_safe, img_path, i + 1): i
            for i, img_path in enumerate(invoice_images)
        }

        # æ”¶é›†çµæœä¸¦ä¿æŒåŸå§‹é †åº
        completed_count = 0
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            results[index] = future.result()
            completed_count += 1
            progress = (completed_count / num_invoices) * 100
            print(f"  ğŸ“Š é€²åº¦: {completed_count}/{num_invoices} ({progress:.1f}%)")

    processing_time = time.time() - start_time
    print(f"ğŸ‰ å¤šç·šç¨‹è™•ç†å®Œæˆï¼è€—æ™‚: {processing_time:.2f}ç§’ (å¹³å‡: {processing_time/num_invoices:.2f}ç§’/å¼µ)")

    df = pd.DataFrame(results)
    report_filename = f'ocr_report_{int(time.time())}.xlsx'
    report_path = os.path.join(app.config['REPORTS_FOLDER'], report_filename)
    df.to_excel(report_path, index=False)
    print(f"å ±å‘Šå·²ç”¢ç”Ÿ: {report_path}")
    if os.path.exists(app.config['TEMP_IMG_FOLDER']): shutil.rmtree(app.config['TEMP_IMG_FOLDER'])
    if os.path.exists(app.config['CROPPED_RECEIPTS_FOLDER']): shutil.rmtree(app.config['CROPPED_RECEIPTS_FOLDER'])
    return report_path, results

# ======================================================================
# --- è¼”åŠ©å‡½å¼ ---
# ======================================================================
def get_origin_city(origin):
    """å¾å®Œæ•´çš„å‡ºç™¼åœ°åœ°å€ä¸­ï¼Œæå–å‡ºç¸£å¸‚åç¨±ã€‚"""
    city_keywords = ["å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "æ¡ƒåœ’å¸‚", "å°ä¸­å¸‚", "å°å—å¸‚", "é«˜é›„å¸‚", "åŸºéš†å¸‚", "æ–°ç«¹å¸‚", "å˜‰ç¾©å¸‚", "å®œè˜­ç¸£", "æ–°ç«¹ç¸£", "è‹—æ —ç¸£", "å½°åŒ–ç¸£", "å—æŠ•ç¸£", "é›²æ—ç¸£", "å˜‰ç¾©ç¸£", "å±æ±ç¸£", "èŠ±è“®ç¸£", "å°æ±ç¸£", "æ¾æ¹–ç¸£", "é‡‘é–€ç¸£", "é€£æ±Ÿç¸£"]
    for city in city_keywords:
        if city in origin:
            return city
    return ""

# ======================================================================
# --- API Models for Swagger Documentation ---
# ======================================================================

# API Models
hello_model = api.model('HelloResponse', {
    'message': fields.String(required=True, description='Response message')
})

material_model = api.model('Material', {
    'material_id': fields.String(required=True, description='Material ID'),
    'material_name': fields.String(required=True, description='Material name'),
    'carbon_footprint': fields.Float(required=True, description='Carbon footprint value'),
    'declaration_unit': fields.String(required=True, description='Declaration unit'),
    'data_source': fields.String(description='Data source'),
    'life_cycle_scope': fields.String(description='Life cycle scope'),
    'announcement_year': fields.Integer(description='Announcement year'),
    'verified': fields.String(description='Verified by'),
    'remarks': fields.String(description='Remarks')
})

material_match_model = api.model('MaterialMatch', {
    'name': fields.String(required=True, description='Material name'),
    'id': fields.String(required=True, description='Material ID'),
    'carbon_footprint': fields.Float(required=True, description='Carbon footprint value'),
    'declaration_unit': fields.String(required=True, description='Declaration unit'),
    'score': fields.Float(required=True, description='Match score')
})

material_create_model = api.model('MaterialCreate', {
    'material_name': fields.String(required=True, description='Material name'),
    'carbon_footprint': fields.Float(required=True, description='Carbon footprint value'),
    'declaration_unit': fields.String(required=True, description='Declaration unit'),
    'data_source': fields.String(description='Data source'),
    'life_cycle_scope': fields.String(description='Life cycle scope'),
    'announcement_year': fields.Integer(description='Announcement year'),
    'verified': fields.String(description='Verified by'),
    'remarks': fields.String(description='Remarks')
})

success_response_model = api.model('SuccessResponse', {
    'success': fields.Boolean(required=True, description='Success status'),
    'data': fields.Raw(description='Response data'),
    'message': fields.String(description='Response message')
})

material_batch_result_model = api.model('MaterialBatchResult', {
    'query': fields.String(required=True, description='Original query'),
    'matches': fields.List(fields.Nested(material_match_model)),
    'default': fields.Integer(description='Default selection index')
})

gmap_request_model = api.model('GMapRequest', {
    'origin': fields.String(required=True, description='Starting location'),
    'destinations': fields.String(required=True, description='Destinations (newline separated)')
})

gmap_result_model = api.model('GMapResult', {
    'origin': fields.String(required=True, description='Starting location'),
    'destination': fields.String(required=True, description='Destination'),
    'distance': fields.String(required=True, description='Distance information'),
    'image_filename': fields.String(description='Screenshot filename'),
    'screenshot_url': fields.String(description='Screenshot URL')
})

gmap_response_model = api.model('GMapResponse', {
    'results': fields.List(fields.Nested(gmap_result_model)),
    'session_id': fields.String(required=True, description='Session ID for downloads')
})

ocr_response_model = api.model('OCRResponse', {
    'message': fields.String(required=True, description='Processing status message'),
    'download_url': fields.String(required=True, description='Download URL for Excel report'),
    'data': fields.List(fields.Raw, description='OCR extracted data')
})

error_model = api.model('Error', {
    'error': fields.String(required=True, description='Error message')
})

# API Namespaces
ns_general = api.namespace('general', description='General operations')
ns_materials = api.namespace('materials', description='Material matching operations')
ns_gmap = api.namespace('gmap', description='Google Maps operations')
ns_ocr = api.namespace('ocr', description='OCR processing operations')

# ======================================================================
# --- API ç«¯é» (Endpoints) ---
# ======================================================================

@app.route('/')
def index():
    return jsonify({
        "message": "MFish Station Backend API",
        "status": "running",
        "swagger_docs": "/docs/",
        "endpoints": {
            "hello": "/api/general/hello",
            "materials": "/api/materials/match-batch", 
            "gmap": "/api/gmap/process",
            "ocr": "/api/ocr/process-pdf"
        }
    })

@ns_general.route('/hello')
class HelloWorld(Resource):
    @ns_general.doc('hello_world')
    @ns_general.marshal_with(hello_model)
    def get(self):
        """Test API connection and database status"""
        if supabase: 
            return {"message": "å“ˆå›‰ï¼æˆ‘ä¾†è‡ªæˆåŠŸé€£ç·šåˆ° Supabase çš„ Python å¾Œç«¯ï¼"}
        else: 
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")

@ns_general.route('/health')
class HealthCheck(Resource):
    @ns_general.doc('health_check')
    def get(self):
        """Check service health and status"""
        try:
            # Test database connection
            db_status = 'connected'
            if supabase:
                try:
                    # Simple test query to verify connection
                    test_response = supabase.table('materials').select('material_id').limit(1).execute()
                    db_status = 'connected'
                except:
                    db_status = 'disconnected'
            else:
                db_status = 'disconnected'
            
            # Check other services
            gmaps_status = 'available' if GOOGLE_MAPS_API_KEY and gmaps else 'unavailable'
            ocr_status = 'available'  # OCR is commented out but available
            
            return {
                "success": True,
                "data": {
                    "status": "healthy",
                    "timestamp": datetime.now().isoformat(),
                    "services": {
                        "database": db_status,
                        "google_maps": gmaps_status,
                        "ocr": ocr_status
                    }
                }
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Health check failed: {str(e)}"
            }, 500

@ns_general.route('/info')
class ServiceInfo(Resource):
    @ns_general.doc('service_info')
    def get(self):
        """Get service information"""
        return {
            "success": True,
            "data": {
                "name": "MFish Station Backend API",
                "version": "1.0",
                "description": "Backend API for MFish Station - OCR, Google Maps, and Material Management",
                "endpoints": {
                    "health": "/api/general/health",
                    "materials": "/api/materials/",
                    "gmap": "/api/gmap/process",
                    "ocr": "/api/ocr/process-pdf"
                }
            }
        }

@ns_materials.route('/match-batch')
class MaterialMatchBatch(Resource):
    @ns_materials.doc('match_materials_batch')
    @ns_materials.expect(api.model('MaterialQueries', {
        'queries': fields.List(fields.String, required=True, description='List of material names to search')
    }))
    @ns_materials.marshal_with(success_response_model)
    def post(self):
        """Batch match materials against database"""
        if not supabase: 
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        data = request.get_json()
        queries = data.get('queries', []) if data else []
        if not queries: 
            api.abort(400, "æ²’æœ‰æ”¶åˆ°ä»»ä½•æŸ¥è©¢è³‡æ–™")
        
        all_results = []
        try:
            for original_name in queries:
                # ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨±é€²è¡Œæœå°‹
                response = supabase.table('materials').select('material_id, material_name, carbon_footprint, declaration_unit, data_source').ilike('material_name', f'%{original_name}%').limit(5).execute()
                search_results = response.data if response.data else []
                
                # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆå‰ç«¯æœŸæœ›
                formatted_matches = []
                for material in search_results:
                    formatted_matches.append({
                        "name": material.get('material_name', ''),
                        "id": material.get('material_id', ''),
                        "carbon_footprint": material.get('carbon_footprint', 0),
                        "declaration_unit": material.get('declaration_unit', ''),
                        "data_source": material.get('data_source', ''),
                        "score": 0.8  # æš«æ™‚çµ¦ä¸€å€‹å›ºå®šåˆ†æ•¸
                    })
                
                all_results.append({ 
                    "query": original_name,
                    "matches": formatted_matches,
                    "default": 0 if formatted_matches else None
                })
            
            return {
                "success": True,
                "data": all_results
            }
        except Exception as e:
            print(f"æ‰¹æ¬¡æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"æ‰¹æ¬¡æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# Legacy route for backward compatibility
@app.route('/materials/match-batch', methods=['POST'])
def match_materials_batch():
    if not supabase: return jsonify({"success": False, "error": "è³‡æ–™åº«é€£ç·šå¤±æ•—"}), 500
    data = request.get_json()
    queries = data.get('queries', []) if data else []
    if not queries: return jsonify({"success": False, "error": "æ²’æœ‰æ”¶åˆ°ä»»ä½•æŸ¥è©¢è³‡æ–™"}), 400
    all_results = []
    try:
        for original_name in queries:
            response = supabase.table('materials').select('material_id, material_name, carbon_footprint, declaration_unit, data_source').ilike('material_name', f'%{original_name}%').limit(5).execute()
            search_results = response.data if response.data else []
            formatted_matches = []
            for material in search_results:
                formatted_matches.append({
                    "name": material.get('material_name', ''),
                    "id": material.get('material_id', ''),
                    "carbon_footprint": material.get('carbon_footprint', 0),
                    "declaration_unit": material.get('declaration_unit', ''),
                    "data_source": material.get('data_source', ''),
                    "score": 0.8
                })
            all_results.append({ 
                "query": original_name,
                "matches": formatted_matches,
                "default": 0 if formatted_matches else None
            })
        return jsonify({"success": True, "data": all_results})
    except Exception as e:
        print(f"æ‰¹æ¬¡æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return jsonify({"success": False, "error": f"æ‰¹æ¬¡æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}), 500

@ns_materials.route('/all')
class MaterialsAll(Resource):
    @ns_materials.doc('get_all_materials')
    @ns_materials.marshal_with(success_response_model)
    def get(self):
        """Get all materials from database using the material service"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        try:
            # Import and use the material service
            from services.material_service import MaterialService
            material_service = MaterialService(supabase)
            
            print("ğŸ”„ Using MaterialService to fetch all materials...")
            all_materials = material_service.get_all_materials()
            
            return {
                "success": True,
                "data": all_materials,
                "total_count": len(all_materials)
            }
        except Exception as e:
            print(f"ç²å–æ‰€æœ‰ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"ç²å–ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

@ns_materials.route('/count')
class MaterialsCount(Resource):
    @ns_materials.doc('get_materials_count')
    def get(self):
        """Get total count of materials in database"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        try:
            # Get exact count using Supabase count feature
            response = supabase.table('materials').select('*', count='exact').limit(1).execute()
            
            return {
                "success": True,
                "count": response.count,
                "message": f"è³‡æ–™åº«ä¸­å…±æœ‰ {response.count} ç­†ææ–™è¨˜éŒ„"
            }
        except Exception as e:
            print(f"ç²å–ææ–™æ•¸é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"ç²å–ææ–™æ•¸é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

@ns_materials.route('/search')
class MaterialsSearch(Resource):
    @ns_materials.doc('search_materials')
    @ns_materials.marshal_with(success_response_model)
    def get(self):
        """Search materials by query"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        parser = reqparse.RequestParser()
        parser.add_argument('q', type=str, required=True, help='Search query')
        parser.add_argument('limit', type=int, default=5, help='Result limit')
        args = parser.parse_args()
        
        query = args['q'].strip()
        limit = args['limit']
        
        if not query:
            api.abort(400, "æœå°‹æŸ¥è©¢ä¸èƒ½ç‚ºç©º")
        
        try:
            # Search in material_name column with partial matching
            response = supabase.table('materials').select('*').ilike('material_name', f'%{query}%').limit(limit).execute()
            materials = response.data if response.data else []
            
            return {
                "success": True,
                "data": materials
            }
        except Exception as e:
            print(f"æœå°‹ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"æœå°‹ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

@ns_materials.route('')
class Materials(Resource):
    @ns_materials.doc('create_material')
    @ns_materials.expect(material_create_model)
    @ns_materials.marshal_with(success_response_model)
    def post(self):
        """Create a new material"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        data = request.get_json()
        if not data:
            api.abort(400, "è«‹æä¾›ææ–™æ•¸æ“š")
        
        # Validate required fields
        required_fields = ['material_name', 'carbon_footprint', 'declaration_unit']
        for field in required_fields:
            if not data.get(field):
                api.abort(400, f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
        
        try:
            # Insert material into database
            material_data = {
                'material_name': data['material_name'],
                'carbon_footprint': float(data['carbon_footprint']),
                'declaration_unit': data['declaration_unit'],
                'data_source': data.get('data_source', ''),
                'life_cycle_scope': data.get('life_cycle_scope', ''),
                'announcement_year': int(data['announcement_year']) if data.get('announcement_year') else None,
                'verified': data.get('verified', ''),
                'remarks': data.get('remarks', '')
            }
            
            response = supabase.table('materials').insert(material_data).execute()
            
            if response.data:
                return {
                    "success": True,
                    "data": response.data[0] if response.data else material_data,
                    "message": "ææ–™å‰µå»ºæˆåŠŸ"
                }
            else:
                api.abort(500, "å‰µå»ºææ–™å¤±æ•—")
                
        except Exception as e:
            print(f"å‰µå»ºææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"å‰µå»ºææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

@ns_materials.route('/<string:material_id>')
class MaterialById(Resource):
    @ns_materials.doc('get_material_by_id')
    @ns_materials.marshal_with(success_response_model)
    def get(self, material_id):
        """Get material by ID"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        try:
            response = supabase.table('materials').select('*').eq('material_id', material_id).execute()
            
            if response.data and len(response.data) > 0:
                return {
                    "success": True,
                    "data": response.data[0]
                }
            else:
                api.abort(404, "ææ–™æœªæ‰¾åˆ°")
                
        except Exception as e:
            print(f"ç²å–ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"ç²å–ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    @ns_materials.doc('update_material')
    @ns_materials.expect(material_create_model)
    @ns_materials.marshal_with(success_response_model)
    def put(self, material_id):
        """Update material by ID"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        data = request.get_json()
        if not data:
            api.abort(400, "è«‹æä¾›æ›´æ–°æ•¸æ“š")
        
        try:
            # Prepare update data (only include provided fields)
            update_data = {}
            allowed_fields = ['material_name', 'carbon_footprint', 'declaration_unit', 
                            'data_source', 'life_cycle_scope', 'announcement_year', 'verified', 'remarks']
            
            for field in allowed_fields:
                if field in data:
                    if field == 'carbon_footprint' and data[field] is not None:
                        update_data[field] = float(data[field])
                    elif field == 'announcement_year' and data[field] is not None:
                        update_data[field] = int(data[field])
                    else:
                        update_data[field] = data[field]
            
            if not update_data:
                api.abort(400, "æ²’æœ‰æä¾›æœ‰æ•ˆçš„æ›´æ–°æ•¸æ“š")
            
            response = supabase.table('materials').update(update_data).eq('material_id', material_id).execute()
            
            if response.data and len(response.data) > 0:
                return {
                    "success": True,
                    "data": response.data[0],
                    "message": "ææ–™æ›´æ–°æˆåŠŸ"
                }
            else:
                api.abort(404, "ææ–™æœªæ‰¾åˆ°æˆ–æ›´æ–°å¤±æ•—")
                
        except Exception as e:
            print(f"æ›´æ–°ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"æ›´æ–°ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    @ns_materials.doc('delete_material')
    @ns_materials.marshal_with(success_response_model)
    def delete(self, material_id):
        """Delete material by ID"""
        if not supabase:
            api.abort(500, "è³‡æ–™åº«é€£ç·šå¤±æ•—")
        
        try:
            # Check if material exists first
            check_response = supabase.table('materials').select('material_id').eq('material_id', material_id).execute()
            
            if not check_response.data or len(check_response.data) == 0:
                api.abort(404, "ææ–™æœªæ‰¾åˆ°")
            
            # Delete the material
            response = supabase.table('materials').delete().eq('material_id', material_id).execute()
            
            return {
                "success": True,
                "data": None,
                "message": "ææ–™åˆªé™¤æˆåŠŸ"
            }
                
        except Exception as e:
            print(f"åˆªé™¤ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"åˆªé™¤ææ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# ============================================================================
# Excel Import/Export Endpoints (Plain Flask Routes)
# ============================================================================

@app.route('/api/materials/template', methods=['GET'])
def download_excel_template():
    """Download Excel template for material import"""
    try:
        # Create template data with all columns
        template_data = {
            'material_name': ['æ··å‡åœŸ (ç¯„ä¾‹)', 'é‹¼ç­‹ (ç¯„ä¾‹)', ''],
            'carbon_footprint': [320.5, 1850.0, ''],
            'declaration_unit': ['kg/mÂ³', 'kg/kg', ''],
            'data_source': ['ç’°ä¿ç½²è³‡æ–™ (é¸å¡«)', 'ISOæ¨™æº– (é¸å¡«)', ''],
            'announcement_year': [2023, 2022, ''],
            'life_cycle_scope': ['A1-A3', 'A1-A5', ''],
            'verified': ['æ˜¯', 'å¦', ''],
            'remarks': ['å‚™è¨»èªªæ˜ (é¸å¡«)', 'å¦ä¸€å€‹ç¯„ä¾‹', '']
        }

        df = pd.DataFrame(template_data)
        output = io.BytesIO()

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Write data to Excel
            df.to_excel(writer, index=False, sheet_name='ææ–™åŒ¯å…¥ç¯„æœ¬')

            # Get workbook and worksheet for formatting
            workbook = writer.book
            worksheet = writer.sheets['ææ–™åŒ¯å…¥ç¯„æœ¬']

            # Define styles
            from openpyxl.styles import Font, PatternFill, Border, Side

            # Header style - required columns
            required_fill = PatternFill(start_color="FFE6E6", end_color="FFE6E6", fill_type="solid")
            required_font = Font(bold=True, color="CC0000")

            # Header style - optional columns
            optional_fill = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
            optional_font = Font(bold=True, color="0066CC")

            # Apply styles to headers
            required_columns = ['material_name', 'carbon_footprint', 'declaration_unit']

            for col_num, column in enumerate(df.columns, 1):
                cell = worksheet.cell(row=1, column=col_num)
                if column in required_columns:
                    cell.fill = required_fill
                    cell.font = required_font
                else:
                    cell.fill = optional_fill
                    cell.font = optional_font

                # Auto-adjust column width
                worksheet.column_dimensions[cell.column_letter].width = 20

        output.seek(0)

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name='ææ–™åŒ¯å…¥ç¯„æœ¬.xlsx'
        )

    except Exception as e:
        print(f"Error creating template: {e}")
        return jsonify({"error": f"Failed to create template: {str(e)}"}), 500

@app.route('/api/materials/preview-excel', methods=['POST'])
def preview_excel_materials():
    """Preview Excel file contents before import"""
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400

    file = request.files['file']
    if file.filename == '' or not file.filename.lower().endswith(('.xlsx', '.xls')):
        return jsonify({"error": "Please provide a valid Excel file"}), 400

    try:
        # Windows compatibility fix: Read file into memory
        print(f"Reading Excel file: {file.filename}")
        file_content = file.read()
        file.seek(0)  # Reset pointer

        # Create BytesIO object
        excel_buffer = io.BytesIO(file_content)

        # Determine engine based on file extension
        filename = file.filename.lower()
        engine = 'openpyxl' if filename.endswith('.xlsx') else 'xlrd'

        print(f"Using pandas engine: {engine}")

        # Read Excel file with explicit engine
        df = pd.read_excel(excel_buffer, engine=engine)

        print(f"Successfully read {len(df)} rows from Excel")

        # Validate required columns
        required_columns = ['material_name', 'carbon_footprint', 'declaration_unit']
        optional_columns = ['data_source', 'announcement_year', 'life_cycle_scope', 'verified', 'remarks']

        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return jsonify({
                "error": f"Missing required columns: {', '.join(missing_columns)}. Required: {', '.join(required_columns)}"
            }), 400

        # Process and validate data
        preview_data = []
        validation_errors = []

        for index, row in df.iterrows():
            row_data = {}
            row_errors = []

            try:
                # Required fields
                row_data['material_name'] = str(row['material_name']).strip()
                if not row_data['material_name'] or row_data['material_name'].lower() == 'nan':
                    row_errors.append("Material name cannot be empty")

                try:
                    row_data['carbon_footprint'] = float(row['carbon_footprint'])
                    if row_data['carbon_footprint'] < 0:
                        row_errors.append("Carbon footprint must be non-negative")
                except (ValueError, TypeError):
                    row_errors.append("Invalid carbon footprint value")
                    row_data['carbon_footprint'] = None

                row_data['declaration_unit'] = str(row['declaration_unit']).strip()
                if not row_data['declaration_unit'] or row_data['declaration_unit'].lower() == 'nan':
                    row_errors.append("Declaration unit cannot be empty")

                # Optional fields
                for field in optional_columns:
                    if field in df.columns and pd.notna(row[field]):
                        if field == 'announcement_year':
                            try:
                                row_data[field] = int(float(row[field]))
                                if row_data[field] < 1900 or row_data[field] > 2100:
                                    row_errors.append("Invalid announcement year")
                            except (ValueError, TypeError):
                                row_errors.append("Invalid announcement year format")
                                row_data[field] = None
                        else:
                            row_data[field] = str(row[field]).strip()
                    else:
                        row_data[field] = ''

                # Add row index and validation status
                row_data['row_index'] = index + 2  # Excel rows start at 2 (accounting for header)
                row_data['is_valid'] = len(row_errors) == 0
                row_data['errors'] = row_errors

                preview_data.append(row_data)

                if row_errors:
                    validation_errors.extend([f"Row {index + 2}: {error}" for error in row_errors])

            except Exception as e:
                error_msg = f"Row {index + 2}: Unexpected error - {str(e)}"
                print(f"Error processing row {index + 2}: {e}")
                validation_errors.append(error_msg)
                preview_data.append({
                    'row_index': index + 2,
                    'is_valid': False,
                    'errors': [f"Unexpected error: {str(e)}"],
                    **{col: '' for col in required_columns + optional_columns}
                })

        # Calculate statistics
        valid_count = sum(1 for item in preview_data if item['is_valid'])
        invalid_count = len(preview_data) - valid_count

        print(f"Preview complete: {valid_count} valid, {invalid_count} invalid rows")

        return jsonify({
            "preview_data": preview_data,
            "total_rows": len(preview_data),
            "valid_rows": valid_count,
            "invalid_rows": invalid_count,
            "validation_errors": validation_errors[:20],  # Limit to first 20 errors
            "columns": {
                "required": required_columns,
                "optional": optional_columns
            }
        })

    except Exception as e:
        error_msg = f"Failed to preview Excel file: {str(e)}"
        print(f"Error previewing Excel: {e}")
        import traceback
        traceback.print_exc()  # Print full traceback for debugging
        return jsonify({"error": error_msg}), 500

@app.route('/api/materials/import-excel', methods=['POST'])
def import_materials_from_excel():
    """Import materials from previewed Excel data"""
    if not supabase:
        return jsonify({"error": "Database connection not available"}), 500

    try:
        # Import and instantiate MaterialService
        from services.material_service import MaterialService
        material_service = MaterialService(supabase)

        data = request.get_json()
        if not data or 'materials' not in data:
            return jsonify({"error": "No materials data provided"}), 400

        materials_data = data['materials']
        if not isinstance(materials_data, list):
            return jsonify({"error": "Materials data must be an array"}), 400

        # Filter only valid materials
        valid_materials = [material for material in materials_data if material.get('is_valid', False)]

        if not valid_materials:
            return jsonify({"error": "No valid materials to import"}), 400

        # Import materials
        imported_count = 0
        error_count = 0
        errors = []

        for material in valid_materials:
            try:
                # Prepare material data for database
                material_data = {
                    'material_name': material['material_name'],
                    'carbon_footprint': material['carbon_footprint'],
                    'declaration_unit': material['declaration_unit'],
                }

                # Add optional fields if they exist and are not empty
                optional_fields = ['data_source', 'announcement_year', 'life_cycle_scope', 'verified', 'remarks']
                for field in optional_fields:
                    if field in material and material[field] and str(material[field]).strip():
                        if field == 'announcement_year':
                            material_data[field] = int(material[field])
                        else:
                            material_data[field] = str(material[field]).strip()

                # Create material in database
                material_service.create_material(material_data)
                imported_count += 1

            except Exception as e:
                error_count += 1
                row_index = material.get('row_index', 'unknown')
                errors.append(f"Row {row_index}: {str(e)}")

        return jsonify({
            "message": f"Import completed. {imported_count} materials imported, {error_count} errors.",
            "imported_count": imported_count,
            "error_count": error_count,
            "errors": errors[:10]  # Limit to first 10 errors
        })

    except Exception as e:
        print(f"Error importing materials: {e}")
        return jsonify({"error": f"Failed to import materials: {str(e)}"}), 500

@ns_gmap.route('/process')
class GMapProcess(Resource):
    @ns_gmap.doc('gmap_process')
    @ns_gmap.expect(gmap_request_model)
    @ns_gmap.marshal_with(gmap_response_model)
    def post(self):
        """Process Google Maps routes and generate screenshots"""
        data = request.get_json()
        origin, destinations_text = data.get('origin'), data.get('destinations', '')
        if not origin or not destinations_text: 
            api.abort(400, "å¿…é ˆæä¾›å‡ºç™¼åœ°å’Œç›®çš„åœ°ã€‚")
        
        destinations = [addr.strip() for addr in destinations_text.split('\n') if addr.strip()]
        session_id = f"session_{int(time.time())}"
        today_str = datetime.now().strftime("%Y-%m-%d")
        image_folder_path = os.path.join(app.config['SCREENSHOTS_FOLDER'], today_str, session_id)
        os.makedirs(image_folder_path, exist_ok=True)
        
        try:
            # ä½¿ç”¨Google Mapsæ©Ÿå™¨äºº
            robot = GoogleMapsRobot(headless=True)
            robot_results = robot.process_multiple_routes(origin, destinations, image_folder_path)
            
            # è½‰æ›çµæœæ ¼å¼ä»¥ç¬¦åˆå‰ç«¯æœŸæœ›
            results = []
            for robot_result in robot_results:
                if "image_filename" in robot_result:
                    screenshot_url = f"screenshots/{today_str}/{session_id}/{robot_result['image_filename']}"
                else:
                    screenshot_url = ""
                
                results.append({
                    "origin": robot_result["origin"],
                    "destination": robot_result["destination"], 
                    "distance": robot_result["distance"],
                    "image_filename": robot_result.get("image_filename", ""),
                    "image_local_path": robot_result.get("image_local_path", ""),
                    "screenshot_url": screenshot_url
                })
            
            SESSION_RESULTS_CACHE[session_id] = results
            return {"results": results, "session_id": session_id}
            
        except Exception as e:
            print(f"è™•ç† Google Maps æ©Ÿå™¨äººæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            api.abort(500, f"è™•ç†æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")

@app.route('/api/download/excel/<session_id>', methods=['GET'])
def download_excel(session_id):
    session_data = SESSION_RESULTS_CACHE.get(session_id)
    if not session_data: return "Session not found or expired.", 404
    export_data = [{"èµ·å§‹é»": item['origin'], "çµ‚é»": item['destination'], "è·é›¢": item['distance'], "åœ–ç‰‡åç¨±": item['image_filename']} for item in session_data]
    df = pd.DataFrame(export_data)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer: df.to_excel(writer, index=False, sheet_name='è·¯ç·šè·é›¢å ±å‘Š')
    output.seek(0)
    return send_file(output, mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', as_attachment=True, download_name=f'google_maps_report_{session_id}.xlsx')

@app.route('/api/download/zip/<session_id>', methods=['GET'])
def download_zip(session_id):
    session_data = SESSION_RESULTS_CACHE.get(session_id)
    if not session_data: return "Session not found or expired.", 404
    memory_file = io.BytesIO()
    with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zf:
        for item in session_data: zf.write(item['image_local_path'], arcname=item['image_filename'])
    memory_file.seek(0)
    return send_file(memory_file, mimetype='application/zip', as_attachment=True, download_name=f'map_images_{session_id}.zip')

@app.route('/screenshots/<path:path>')
def send_screenshot(path):
    return send_from_directory(app.config['SCREENSHOTS_FOLDER'], path)

@ns_ocr.route('/process-pdf')
class OCRProcessPDF(Resource):
    @ns_ocr.doc('ocr_process_pdf')
    @ns_ocr.expect(api.parser().add_argument('file', location='files', type='file', required=True, help='PDF file to process'))
    @ns_ocr.marshal_with(ocr_response_model)
    def post(self):
        """Process PDF file with OCR to extract invoice information"""
        if 'file' not in request.files: 
            api.abort(400, "æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆéƒ¨åˆ†")
        
        file = request.files['file']
        if file.filename == '' or not file.filename.lower().endswith('.pdf'): 
            api.abort(400, "æœªé¸æ“‡æˆ–é PDF æª”æ¡ˆ")
        
        unique_filename = f"{int(time.time())}_{werkzeug.utils.secure_filename(file.filename)}"
        pdf_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
        file.save(pdf_path)
        
        try:
            report_path, ocr_data = process_invoice_pdf(pdf_path)
            report_filename = os.path.basename(report_path)
            return {
                "message": "OCR è™•ç†å®Œæˆï¼",
                "download_url": f"api/download/ocr-report/{report_filename}",
                "data": ocr_data,
                "total_invoices": len(ocr_data),
                "report_filename": report_filename
            }
        except Exception as e:
            print(f"OCR è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            api.abort(500, f"è™•ç†å¤±æ•—: {str(e)}")
        finally:
            if os.path.exists(pdf_path): 
                os.remove(pdf_path)

@app.route('/api/download/ocr-report/<filename>')
def download_ocr_report(filename):
    return send_from_directory(app.config['REPORTS_FOLDER'], filename, as_attachment=True)

# --- ä¸»ç¨‹å¼é€²å…¥é» ---
if __name__ == '__main__':
    for folder_key in ['SCREENSHOTS_FOLDER', 'UPLOAD_FOLDER', 'REPORTS_FOLDER', 'TEMP_IMG_FOLDER', 'CROPPED_RECEIPTS_FOLDER']:
        folder_path = app.config[folder_key]
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

    # Use PORT environment variable for Railway deployment, fallback to 8001 for local development
    port = int(os.getenv('PORT', 8001))
    # In production (Railway), debug should be False
    debug = os.getenv('FLASK_ENV', 'development') == 'development'

    app.run(debug=debug, port=port, host='0.0.0.0')
